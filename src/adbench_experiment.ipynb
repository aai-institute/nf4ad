{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e5161",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyod'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     roc_auc_score,\n\u001b[1;32m     14\u001b[0m     average_precision_score,\n\u001b[1;32m     15\u001b[0m     precision_recall_curve,\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyod\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mecod\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ECOD\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01musflows\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flow\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Global configuration\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------------\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyod'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Any, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "from pyod.models.ecod import ECOD\n",
    "\n",
    "from usflows import Flow\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "RANDOM_STATE: int = 42\n",
    "DATA_DIR: Path = Path(\"./nf4ad/data/adbench\")  # assumes ./data contains the Classical .npz files\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Dataset resolution & loading (unchanged)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def resolve_npz_path(dataset_name: str, data_dir: Path = DATA_DIR) -> Path:\n",
    "    dataset_name = dataset_name.strip()\n",
    "    data_dir = Path(data_dir)\n",
    "\n",
    "    if not data_dir.is_dir():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Data directory {data_dir.resolve()} does not exist. \"\n",
    "            \"Make sure you created it and copied the ADBench .npz files into it.\"\n",
    "        )\n",
    "\n",
    "    candidate = data_dir / dataset_name\n",
    "    if candidate.is_file():\n",
    "        return candidate\n",
    "\n",
    "    if not dataset_name.endswith(\".npz\"):\n",
    "        candidate_with_ext = data_dir / f\"{dataset_name}.npz\"\n",
    "        if candidate_with_ext.is_file():\n",
    "            return candidate_with_ext\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    if dataset_name.isdigit():\n",
    "        prefix = f\"{dataset_name}_\"\n",
    "        candidates = [p for p in data_dir.glob(\"*.npz\") if p.name.startswith(prefix)]\n",
    "    else:\n",
    "        norm = dataset_name.lower()\n",
    "        for p in data_dir.glob(\"*.npz\"):\n",
    "            stem = p.stem.lower()\n",
    "            if stem == norm:\n",
    "                candidates.append(p)\n",
    "                continue\n",
    "            if \"_\" in stem:\n",
    "                _, suffix = stem.split(\"_\", 1)\n",
    "                if suffix == norm:\n",
    "                    candidates.append(p)\n",
    "\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not match dataset name '{dataset_name}' to any .npz file in \"\n",
    "            f\"{data_dir.resolve()}.\"\n",
    "        )\n",
    "\n",
    "    if len(candidates) > 1:\n",
    "        names = \", \".join(sorted(p.name for p in candidates))\n",
    "        raise RuntimeError(\n",
    "            f\"Dataset name '{dataset_name}' is ambiguous; it matches multiple files: \"\n",
    "            f\"{names}. Please specify a more precise name.\"\n",
    "        )\n",
    "\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "def load_classical_dataset(\n",
    "    dataset_name: str,\n",
    "    data_dir: Path = DATA_DIR,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    npz_path = resolve_npz_path(dataset_name, data_dir)\n",
    "    npz = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "    X = npz[\"X\"]\n",
    "    y = npz[\"y\"].astype(int)\n",
    "\n",
    "    print(\n",
    "        f\"Loaded {npz_path.name}: X.shape={X.shape}, \"\n",
    "        f\"y.shape={y.shape}, anomaly_ratio={y.mean():.4f}\"\n",
    "    )\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Metrics (unchanged)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def evaluate_anomaly_scores(\n",
    "    y_true: ArrayLike,\n",
    "    scores: ArrayLike,\n",
    ") -> Dict[str, float]:\n",
    "    y_true = np.asarray(y_true).astype(int).ravel()\n",
    "    scores = np.asarray(scores, dtype=float).ravel()\n",
    "\n",
    "    if y_true.shape[0] != scores.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"y_true and scores must have the same length, \"\n",
    "            f\"got {y_true.shape[0]} and {scores.shape[0]}.\"\n",
    "        )\n",
    "\n",
    "    if np.unique(y_true).size < 2:\n",
    "        raise ValueError(\n",
    "            \"y_true must contain both normal (0) and anomalous (1) labels.\"\n",
    "        )\n",
    "\n",
    "    metrics: Dict[str, float] = {}\n",
    "    metrics[\"auc_roc\"] = float(roc_auc_score(y_true, scores))\n",
    "    metrics[\"auc_pr\"] = float(average_precision_score(y_true, scores))\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, scores)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "    best_idx = int(np.argmax(f1))\n",
    "    metrics[\"best_f1\"] = float(f1[best_idx])\n",
    "\n",
    "    if thresholds.size > 0 and best_idx < thresholds.size:\n",
    "        metrics[\"best_f1_threshold\"] = float(thresholds[best_idx])\n",
    "    else:\n",
    "        metrics[\"best_f1_threshold\"] = float(\"nan\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Your method stub (unchanged)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def create_flow_prior(latent_dim, device: torch.device):\n",
    "    \"\"\"Helper to create flow prior with specified latent dimension.\"\"\"\n",
    "    from nf4ad.flows import NonUSFlow\n",
    "    import pyro.distributions as dist\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    base_dist = dist.Normal(\n",
    "        torch.zeros(latent_dim).to(device),\n",
    "        torch.ones(latent_dim).to(device)\n",
    "    )\n",
    "    \n",
    "    # Simple MLP conditioner for testing\n",
    "    class SimpleConditioner(nn.Module):\n",
    "        def __init__(self, in_dim, out_dim):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, out_dim),\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    \n",
    "    # from U import \n",
    "    \n",
    "    flow = NonUSFlow(\n",
    "        in_dims=[latent_dim],\n",
    "        device=device,\n",
    "        coupling_blocks=3,\n",
    "        base_distribution=base_dist,\n",
    "        prior_scale=1.0,\n",
    "        affine_conjugation=True,\n",
    "        conditioner_cls=SimpleConditioner,\n",
    "        conditioner_args={\n",
    "            'in_dim': latent_dim,\n",
    "            'out_dim': latent_dim * 2,  # For affine coupling: scale + shift\n",
    "        },\n",
    "        nonlinearity=nn.ReLU(),\n",
    "    )\n",
    "    \n",
    "    return flow\n",
    "\n",
    "\n",
    "def your_method_scores(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Template wrapper for flow anomaly detection method.\n",
    "\n",
    "    Implement s.t.:\n",
    "        1) Fits your model on (X_train, y_train) in the appropriate way\n",
    "           (for unsupervised methods you may ignore y_train).\n",
    "        2) Returns a 1D array of anomaly scores for X_test, where higher\n",
    "           scores mean \"more anomalous\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scores : np.ndarray of shape (n_test,)\n",
    "        Anomaly scores for X_test.\n",
    "    \"\"\"\n",
    "    # Get number of features\n",
    "    n_features = X_train.shape[1]\n",
    "    \n",
    "    from nf4ad.adbench_wrapper import ADBenchVAEFlowTabular\n",
    "    \n",
    "    flow = ADBenchVAEFlowTabular(\n",
    "        flow_prior=create_flow_prior(n_features, device=torch.device('cuda')),\n",
    "        n_features=5,\n",
    "        latent_dim=n_features,\n",
    "    )\n",
    "    vaeflow.fit(X_train, y_train)\n",
    "    return vaeflow.predict_score(X_test)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# PyOD baseline (ECOD)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def run_pyod_ecod_baseline(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run ECOD (Empirical Cumulative Distribution based Outlier Detection)\n",
    "    from PyOD as an unsupervised baseline.\n",
    "\n",
    "    This is one of the unsupervised methods ADBench includes via PyOD,\n",
    "    but here we call it directly through PyOD's modern API.\n",
    "    \"\"\"\n",
    "    clf = ECOD()          # y is ignored in unsupervised PyOD models\n",
    "    clf.fit(X_train)      # fit on train set\n",
    "    scores = clf.decision_function(X_test)  # higher = more anomalous\n",
    "\n",
    "    return np.asarray(scores, dtype=float).ravel()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# High-level helper to run everything on one dataset (slightly tweaked)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def run_single_dataset_example(\n",
    "    dataset_name: str = \"cardio\",\n",
    "    data_dir: Path = DATA_DIR,\n",
    "    use_baseline: bool = True,\n",
    "    method_fn: Callable[[np.ndarray, np.ndarray, np.ndarray], np.ndarray] | None = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "      - load dataset from .npz\n",
    "      - split into train/test\n",
    "      - standardize features\n",
    "      - run either ECOD baseline or your custom method\n",
    "      - compute metrics\n",
    "    \"\"\"\n",
    "    X, y = load_classical_dataset(dataset_name, data_dir=data_dir)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.5,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    if use_baseline:\n",
    "        scores = run_pyod_ecod_baseline(X_train_scaled, y_train, X_test_scaled)\n",
    "        model_used = \"PyOD-ECOD\"\n",
    "    else:\n",
    "        if method_fn is None:\n",
    "            raise ValueError(\n",
    "                \"use_baseline=False but no method_fn was provided. \"\n",
    "                \"Pass your own method wrapper, e.g. method_fn=your_method_scores.\"\n",
    "            )\n",
    "        scores = method_fn(X_train_scaled, y_train, X_test_scaled)\n",
    "        model_used = getattr(method_fn, \"__name__\", \"custom_method\")\n",
    "\n",
    "    metrics = evaluate_anomaly_scores(y_test, scores)\n",
    "\n",
    "    result: Dict[str, Any] = {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"n_train\": int(X_train.shape[0]),\n",
    "        \"n_test\": int(X_test.shape[0]),\n",
    "        \"model\": model_used,\n",
    "        \"metrics\": metrics,\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"\\nResults on dataset='{dataset_name}' using model='{model_used}':\\n\"\n",
    "        f\"  AUC-ROC : {metrics['auc_roc']:.4f}\\n\"\n",
    "        f\"  AUC-PR  : {metrics['auc_pr']:.4f}\\n\"\n",
    "        f\"  best F1 : {metrics['best_f1']:.4f} \"\n",
    "        f\"(at score threshold â‰ˆ {metrics['best_f1_threshold']:.4f})\"\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Example call\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Run example\n",
    "example_result = run_single_dataset_example(\n",
    "    dataset_name=\"cardio\",\n",
    "    use_baseline=False,\n",
    "    method_fn=your_method_scores,\n",
    ")\n",
    "example_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5102d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_single_dataset_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run example\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m example_result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_single_dataset_example\u001b[49m(\n\u001b[1;32m      3\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcardio\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     use_baseline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     method_fn\u001b[38;5;241m=\u001b[39myour_method_scores,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m example_result\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_single_dataset_example' is not defined"
     ]
    }
   ],
   "source": [
    "# Run example\n",
    "example_result = run_single_dataset_example(\n",
    "    dataset_name=\"cardio\",\n",
    "    use_baseline=False,\n",
    "    method_fn=your_method_scores,\n",
    ")\n",
    "example_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b256a05",
   "metadata": {},
   "source": [
    "# Flow Model Benchmarking on ADBench\n",
    "\n",
    "This notebook demonstrates how to use the comprehensive benchmarking system for Flow models on ADBench datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473f6fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faried/.cache/pypoetry/virtualenvs/nf4ad-ewA-z3hl-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nf4ad.adbench_benchmark import (\n",
    "    ADBenchBenchmark,\n",
    "    BenchmarkConfig,\n",
    "    FlowConfig,\n",
    "    ADBENCH_CLASSICAL_DATASETS,\n",
    ")\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23505577",
   "metadata": {},
   "source": [
    "## 1. Quick Single Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da2c653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running experiment on 6_cardio\n",
      "============================================================\n",
      "Dataset: 6_cardio\n",
      "Features: 21\n",
      "Train: 915 samples (88 anomalies)\n",
      "Test: 916 samples (88 anomalies)\n",
      "Training Flow model on 915 samples...\n",
      "Input shape: torch.Size([915, 21])\n",
      "Device: cpu\n",
      "Epoch 10/100, Loss: 24212999031666069504.0000\n",
      "Early stopping at epoch 15/100\n",
      "Restored best model with loss: 9445856125663156224.0000\n",
      "Training completed. Final loss: 11029379515657467904.0000\n",
      "\n",
      "Results:\n",
      "  ROC-AUC: 0.8022\n",
      "  PR-AUC:  0.3818\n",
      "  Best F1: 0.4247\n",
      "  Training time: 5.30s\n",
      "  Epochs: 15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>n_features</th>\n",
       "      <th>n_train</th>\n",
       "      <th>n_test</th>\n",
       "      <th>n_anomalies_test</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>training_time</th>\n",
       "      <th>inference_time</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>config_coupling_blocks</th>\n",
       "      <th>config_hidden_dim</th>\n",
       "      <th>config_lr</th>\n",
       "      <th>config_batch_size</th>\n",
       "      <th>config_epochs</th>\n",
       "      <th>config_patience</th>\n",
       "      <th>config_min_delta</th>\n",
       "      <th>config_gradient_clip</th>\n",
       "      <th>config_clamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6_cardio</td>\n",
       "      <td>21</td>\n",
       "      <td>915</td>\n",
       "      <td>916</td>\n",
       "      <td>88</td>\n",
       "      <td>0.802207</td>\n",
       "      <td>0.381838</td>\n",
       "      <td>0.424658</td>\n",
       "      <td>5.304112</td>\n",
       "      <td>0.011694</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>None</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  n_features  n_train  n_test  n_anomalies_test   roc_auc  \\\n",
       "0  6_cardio          21      915     916                88  0.802207   \n",
       "\n",
       "     pr_auc   best_f1  training_time  inference_time  n_epochs  \\\n",
       "0  0.381838  0.424658       5.304112        0.011694        15   \n",
       "\n",
       "   config_coupling_blocks  config_hidden_dim  config_lr  config_batch_size  \\\n",
       "0                       8                128      0.001                 64   \n",
       "\n",
       "   config_epochs  config_patience  config_min_delta config_gradient_clip  \\\n",
       "0            100                3            0.0001                 None   \n",
       "\n",
       "   config_clamp  \n",
       "0           1.5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure benchmark\n",
    "config = BenchmarkConfig(\n",
    "    data_dir=Path(\"./nf4ad/data/adbench\"),\n",
    "    output_dir=Path(\"./results/adbench\"),\n",
    "    device=\"cpu\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Configure flow model\n",
    "flow_config = FlowConfig(\n",
    "    coupling_blocks=8,\n",
    "    hidden_dim=128,\n",
    "    lr=1e-3,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    patience=3,\n",
    "    clamp=1.5\n",
    ")\n",
    "\n",
    "# Run on single dataset\n",
    "benchmark = ADBenchBenchmark(config)\n",
    "results = benchmark.run_on_datasets(\n",
    "    datasets=[\"6_cardio\"],\n",
    "    flow_config=flow_config,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "benchmark.results_to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c78cdf",
   "metadata": {},
   "source": [
    "## 2. Multiple Datasets Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b23578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select datasets to test\n",
    "test_datasets = [\n",
    "    \"6_cardio\",\n",
    "    \"2_annthyroid\",\n",
    "    \"38_thyroid\",\n",
    "    \"4_breastw\",\n",
    "    \"18_Ionosphere\",\n",
    "]\n",
    "\n",
    "# Run benchmark\n",
    "benchmark = ADBenchBenchmark(config)\n",
    "results = benchmark.run_on_datasets(\n",
    "    datasets=test_datasets,\n",
    "    flow_config=flow_config,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "df = benchmark.results_to_dataframe()\n",
    "print(df[['dataset', 'roc_auc', 'pr_auc', 'best_f1', 'training_time', 'n_epochs']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6fe83",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'coupling_blocks': [4, 8, 12],\n",
    "    'hidden_dim': [64, 128, 256],\n",
    "    'lr': [1e-4, 1e-3, 1e-2],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [100],\n",
    "    'patience': [10],\n",
    "}\n",
    "\n",
    "# Select datasets for tuning\n",
    "tune_datasets = [\"6_cardio\", \"2_annthyroid\", \"38_thyroid\"]\n",
    "\n",
    "# Run hyperparameter search\n",
    "benchmark = ADBenchBenchmark(config)\n",
    "results_df = benchmark.hyperparameter_search(\n",
    "    datasets=tune_datasets,\n",
    "    param_grid=param_grid,\n",
    "    n_trials=10,  # Try 10 random configurations\n",
    ")\n",
    "\n",
    "# Show top configurations\n",
    "print(\"Top 10 configurations by ROC-AUC:\")\n",
    "top_configs = results_df.nlargest(10, 'roc_auc')\n",
    "print(top_configs[[\n",
    "    'dataset', 'roc_auc', 'pr_auc', 'best_f1',\n",
    "    'config_coupling_blocks', 'config_hidden_dim', 'config_lr', 'config_batch_size'\n",
    "]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bdd9aa",
   "metadata": {},
   "source": [
    "## 4. Analyze Results by Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26415267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by dataset and show statistics\n",
    "summary = results_df.groupby('dataset').agg({\n",
    "    'roc_auc': ['mean', 'std', 'max'],\n",
    "    'pr_auc': ['mean', 'std', 'max'],\n",
    "    'best_f1': ['mean', 'std', 'max'],\n",
    "    'training_time': 'mean',\n",
    "    'n_epochs': 'mean',\n",
    "}).round(4)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc364cd7",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot ROC-AUC by dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=results_df, x='dataset', y='roc_auc')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('ROC-AUC Distribution by Dataset')\n",
    "plt.ylabel('ROC-AUC')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot hyperparameter effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Coupling blocks effect\n",
    "sns.boxplot(data=results_df, x='config_coupling_blocks', y='roc_auc', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Effect of Coupling Blocks')\n",
    "\n",
    "# Hidden dim effect\n",
    "sns.boxplot(data=results_df, x='config_hidden_dim', y='roc_auc', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Effect of Hidden Dimension')\n",
    "\n",
    "# Learning rate effect\n",
    "sns.boxplot(data=results_df, x='config_lr', y='roc_auc', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Effect of Learning Rate')\n",
    "\n",
    "# Batch size effect\n",
    "sns.boxplot(data=results_df, x='config_batch_size', y='roc_auc', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Effect of Batch Size')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0fa4e0",
   "metadata": {},
   "source": [
    "## 6. Training Curves Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac3402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for best performing configurations\n",
    "best_result = benchmark.results[0]  # Get first result\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(best_result.training_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Curve - {best_result.dataset}')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset: {best_result.dataset}\")\n",
    "print(f\"Final ROC-AUC: {best_result.metrics['roc_auc']:.4f}\")\n",
    "print(f\"Training time: {best_result.training_time:.2f}s\")\n",
    "print(f\"Epochs trained: {best_result.n_epochs_trained}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab243bd",
   "metadata": {},
   "source": [
    "## 7. Save and Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b51e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "benchmark.save_results(\"my_experiment\")\n",
    "\n",
    "# Results are saved as:\n",
    "# - JSON (full details with training curves)\n",
    "# - CSV (summary table)\n",
    "\n",
    "# Load and analyze later\n",
    "import json\n",
    "\n",
    "with open(config.output_dir / \"my_experiment.json\") as f:\n",
    "    loaded_results = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(loaded_results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143d357f",
   "metadata": {},
   "source": [
    "## 8. Using USFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with USFlow instead of NonUSFlow\n",
    "config = BenchmarkConfig(\n",
    "    data_dir=Path(\"./nf4ad/data/adbench\"),\n",
    "    output_dir=Path(\"./results/adbench/usflow\"),\n",
    "    device=\"cpu\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Configure USFlow model\n",
    "usflow_config = FlowConfig(\n",
    "    flow_type=\"usflow\",  # Switch to USFlow\n",
    "    coupling_blocks=8,\n",
    "    hidden_dim=128,\n",
    "    lr=1e-3,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    patience=5,\n",
    "    lu_transform=1,\n",
    "    householder=0,\n",
    "    affine_conjugation=True,\n",
    "    prior_scale=1.0,\n",
    "    masktype=\"checkerboard\",\n",
    ")\n",
    "\n",
    "# Run on single dataset\n",
    "benchmark = ADBenchBenchmark(config)\n",
    "results = benchmark.run_on_datasets(\n",
    "    datasets=[\"6_cardio\"],\n",
    "    flow_config=usflow_config,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "benchmark.results_to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c9715",
   "metadata": {},
   "source": [
    "## 9. Compare NonUSFlow vs USFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Run both flow types on same datasets\n",
    "test_datasets = [\"6_cardio\", \"2_annthyroid\"]\n",
    "benchmark = ADBenchBenchmark(config)\n",
    "\n",
    "for flow_type in [\"nonusflow\", \"usflow\"]:\n",
    "    flow_config = FlowConfig(\n",
    "        flow_type=flow_type,\n",
    "        coupling_blocks=8,\n",
    "        hidden_dim=128,\n",
    "        lr=1e-3,\n",
    "        batch_size=64,\n",
    "        epochs=30,\n",
    "        patience=5,\n",
    "    )\n",
    "    \n",
    "    if flow_type == \"usflow\":\n",
    "        flow_config.lu_transform = 1\n",
    "        flow_config.householder = 0\n",
    "        flow_config.masktype = \"checkerboard\"\n",
    "    \n",
    "    benchmark.run_on_datasets(\n",
    "        datasets=test_datasets,\n",
    "        flow_config=flow_config,\n",
    "    )\n",
    "\n",
    "# Compare results\n",
    "df = benchmark.results_to_dataframe()\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, metric in enumerate(['roc_auc', 'pr_auc', 'best_f1']):\n",
    "    sns.barplot(data=df, x='dataset', y=metric, hue='config_flow_type', ax=axes[i])\n",
    "    axes[i].set_title(f'{metric.upper()} Comparison')\n",
    "    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "comparison = df.groupby(['dataset', 'config_flow_type']).agg({\n",
    "    'roc_auc': 'mean',\n",
    "    'pr_auc': 'mean',\n",
    "    'best_f1': 'mean',\n",
    "    'training_time': 'mean',\n",
    "}).round(4)\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13b18e8",
   "metadata": {},
   "source": [
    "## 10. USFlow Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USFlow-specific hyperparameter grid\n",
    "usflow_param_grid = {\n",
    "    'flow_type': ['usflow'],\n",
    "    'coupling_blocks': [4, 8],\n",
    "    'hidden_dim': [64, 128],\n",
    "    'lr': [1e-3, 1e-2],\n",
    "    'batch_size': [32, 64],\n",
    "    'lu_transform': [0, 1],\n",
    "    'householder': [0, 1],\n",
    "    'affine_conjugation': [True, False],\n",
    "    'masktype': ['checkerboard', 'channel'],\n",
    "    'epochs': [50],\n",
    "    'patience': [5],\n",
    "}\n",
    "\n",
    "# Run search\n",
    "tune_datasets = [\"6_cardio\"]\n",
    "benchmark = ADBenchBenchmark(config)\n",
    "results_df = benchmark.hyperparameter_search(\n",
    "    datasets=tune_datasets,\n",
    "    param_grid=usflow_param_grid,\n",
    "    n_trials=8,\n",
    ")\n",
    "\n",
    "# Show best configurations\n",
    "print(\"Top 5 USFlow configurations:\")\n",
    "top = results_df.nlargest(5, 'roc_auc')\n",
    "print(top[[\n",
    "    'roc_auc', 'pr_auc', 'training_time',\n",
    "    'config_lu_transform', 'config_householder', \n",
    "    'config_masktype', 'config_affine_conjugation'\n",
    "]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf4ad-ewA-z3hl-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
