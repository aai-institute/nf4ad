{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e5161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6_cardio.npz: X.shape=(1831, 21), y.shape=(1831,), anomaly_ratio=0.0961\n",
      "Training VAEFlow on 915 samples...\n",
      "Input shape: torch.Size([915, 1, 3, 3])\n",
      "Device: cuda\n",
      "Training completed. Final loss: 200566927941790381113344.0000\n",
      "\n",
      "Results on dataset='cardio' using model='your_method_scores':\n",
      "  AUC-ROC : 0.5125\n",
      "  AUC-PR  : 0.1025\n",
      "  best F1 : 0.1820 (at score threshold ≈ 209428289828075601920.0000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': 'cardio',\n",
       " 'n_train': 915,\n",
       " 'n_test': 916,\n",
       " 'model': 'your_method_scores',\n",
       " 'metrics': {'auc_roc': 0.5125027448397014,\n",
       "  'auc_pr': 0.10251017910372012,\n",
       "  'best_f1': 0.18201754385947477,\n",
       "  'best_f1_threshold': 2.094282898280756e+20}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Any, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "from pyod.models.ecod import ECOD\n",
    "\n",
    "from usflows import Flow\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "RANDOM_STATE: int = 42\n",
    "DATA_DIR: Path = Path(\"./nf4ad/data/adbench\")  # assumes ./data contains the Classical .npz files\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Dataset resolution & loading (unchanged)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def resolve_npz_path(dataset_name: str, data_dir: Path = DATA_DIR) -> Path:\n",
    "    dataset_name = dataset_name.strip()\n",
    "    data_dir = Path(data_dir)\n",
    "\n",
    "    if not data_dir.is_dir():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Data directory {data_dir.resolve()} does not exist. \"\n",
    "            \"Make sure you created it and copied the ADBench .npz files into it.\"\n",
    "        )\n",
    "\n",
    "    candidate = data_dir / dataset_name\n",
    "    if candidate.is_file():\n",
    "        return candidate\n",
    "\n",
    "    if not dataset_name.endswith(\".npz\"):\n",
    "        candidate_with_ext = data_dir / f\"{dataset_name}.npz\"\n",
    "        if candidate_with_ext.is_file():\n",
    "            return candidate_with_ext\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    if dataset_name.isdigit():\n",
    "        prefix = f\"{dataset_name}_\"\n",
    "        candidates = [p for p in data_dir.glob(\"*.npz\") if p.name.startswith(prefix)]\n",
    "    else:\n",
    "        norm = dataset_name.lower()\n",
    "        for p in data_dir.glob(\"*.npz\"):\n",
    "            stem = p.stem.lower()\n",
    "            if stem == norm:\n",
    "                candidates.append(p)\n",
    "                continue\n",
    "            if \"_\" in stem:\n",
    "                _, suffix = stem.split(\"_\", 1)\n",
    "                if suffix == norm:\n",
    "                    candidates.append(p)\n",
    "\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not match dataset name '{dataset_name}' to any .npz file in \"\n",
    "            f\"{data_dir.resolve()}.\"\n",
    "        )\n",
    "\n",
    "    if len(candidates) > 1:\n",
    "        names = \", \".join(sorted(p.name for p in candidates))\n",
    "        raise RuntimeError(\n",
    "            f\"Dataset name '{dataset_name}' is ambiguous; it matches multiple files: \"\n",
    "            f\"{names}. Please specify a more precise name.\"\n",
    "        )\n",
    "\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "def load_classical_dataset(\n",
    "    dataset_name: str,\n",
    "    data_dir: Path = DATA_DIR,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    npz_path = resolve_npz_path(dataset_name, data_dir)\n",
    "    npz = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "    X = npz[\"X\"]\n",
    "    y = npz[\"y\"].astype(int)\n",
    "\n",
    "    print(\n",
    "        f\"Loaded {npz_path.name}: X.shape={X.shape}, \"\n",
    "        f\"y.shape={y.shape}, anomaly_ratio={y.mean():.4f}\"\n",
    "    )\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Metrics (unchanged)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def evaluate_anomaly_scores(\n",
    "    y_true: ArrayLike,\n",
    "    scores: ArrayLike,\n",
    ") -> Dict[str, float]:\n",
    "    y_true = np.asarray(y_true).astype(int).ravel()\n",
    "    scores = np.asarray(scores, dtype=float).ravel()\n",
    "\n",
    "    if y_true.shape[0] != scores.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"y_true and scores must have the same length, \"\n",
    "            f\"got {y_true.shape[0]} and {scores.shape[0]}.\"\n",
    "        )\n",
    "\n",
    "    if np.unique(y_true).size < 2:\n",
    "        raise ValueError(\n",
    "            \"y_true must contain both normal (0) and anomalous (1) labels.\"\n",
    "        )\n",
    "\n",
    "    metrics: Dict[str, float] = {}\n",
    "    metrics[\"auc_roc\"] = float(roc_auc_score(y_true, scores))\n",
    "    metrics[\"auc_pr\"] = float(average_precision_score(y_true, scores))\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, scores)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "    best_idx = int(np.argmax(f1))\n",
    "    metrics[\"best_f1\"] = float(f1[best_idx])\n",
    "\n",
    "    if thresholds.size > 0 and best_idx < thresholds.size:\n",
    "        metrics[\"best_f1_threshold\"] = float(thresholds[best_idx])\n",
    "    else:\n",
    "        metrics[\"best_f1_threshold\"] = float(\"nan\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Your method stub (unchanged)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def create_flow_prior(latent_dim, device: torch.device):\n",
    "    \"\"\"Helper to create flow prior with specified latent dimension.\"\"\"\n",
    "    from nf4ad.flows import NonUSFlow\n",
    "    import pyro.distributions as dist\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    base_dist = dist.Normal(\n",
    "        torch.zeros(latent_dim).to(device),\n",
    "        torch.ones(latent_dim).to(device)\n",
    "    )\n",
    "    \n",
    "    # Simple MLP conditioner for testing\n",
    "    class SimpleConditioner(nn.Module):\n",
    "        def __init__(self, in_dim, out_dim):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, out_dim),\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    \n",
    "    # from U import \n",
    "    \n",
    "    flow = NonUSFlow(\n",
    "        in_dims=[latent_dim],\n",
    "        device=device,\n",
    "        coupling_blocks=3,\n",
    "        base_distribution=base_dist,\n",
    "        prior_scale=1.0,\n",
    "        affine_conjugation=True,\n",
    "        conditioner_cls=SimpleConditioner,\n",
    "        conditioner_args={\n",
    "            'in_dim': latent_dim,\n",
    "            'out_dim': latent_dim * 2,  # For affine coupling: scale + shift\n",
    "        },\n",
    "        nonlinearity=nn.ReLU(),\n",
    "    )\n",
    "    \n",
    "    return flow\n",
    "\n",
    "\n",
    "def your_method_scores(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Template wrapper for flow anomaly detection method.\n",
    "\n",
    "    Implement s.t.:\n",
    "        1) Fits your model on (X_train, y_train) in the appropriate way\n",
    "           (for unsupervised methods you may ignore y_train).\n",
    "        2) Returns a 1D array of anomaly scores for X_test, where higher\n",
    "           scores mean \"more anomalous\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scores : np.ndarray of shape (n_test,)\n",
    "        Anomaly scores for X_test.\n",
    "    \"\"\"\n",
    "    # Get number of features\n",
    "    n_features = X_train.shape[1]\n",
    "    \n",
    "    from nf4ad.adbench_wrapper import ADBenchVAEFlowTabular\n",
    "    \n",
    "    vaeflow = ADBenchVAEFlowTabular(\n",
    "        flow_prior=create_flow_prior(n_features, device=torch.device('cuda')),\n",
    "        n_features=5,\n",
    "        latent_dim=n_features,\n",
    "    )\n",
    "    vaeflow.fit(X_train, y_train)\n",
    "    return vaeflow.predict_score(X_test)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# PyOD baseline (ECOD)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def run_pyod_ecod_baseline(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run ECOD (Empirical Cumulative Distribution based Outlier Detection)\n",
    "    from PyOD as an unsupervised baseline.\n",
    "\n",
    "    This is one of the unsupervised methods ADBench includes via PyOD,\n",
    "    but here we call it directly through PyOD's modern API.\n",
    "    \"\"\"\n",
    "    clf = ECOD()          # y is ignored in unsupervised PyOD models\n",
    "    clf.fit(X_train)      # fit on train set\n",
    "    scores = clf.decision_function(X_test)  # higher = more anomalous\n",
    "\n",
    "    return np.asarray(scores, dtype=float).ravel()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# High-level helper to run everything on one dataset (slightly tweaked)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def run_single_dataset_example(\n",
    "    dataset_name: str = \"cardio\",\n",
    "    data_dir: Path = DATA_DIR,\n",
    "    use_baseline: bool = True,\n",
    "    method_fn: Callable[[np.ndarray, np.ndarray, np.ndarray], np.ndarray] | None = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "      - load dataset from .npz\n",
    "      - split into train/test\n",
    "      - standardize features\n",
    "      - run either ECOD baseline or your custom method\n",
    "      - compute metrics\n",
    "    \"\"\"\n",
    "    X, y = load_classical_dataset(dataset_name, data_dir=data_dir)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.5,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    if use_baseline:\n",
    "        scores = run_pyod_ecod_baseline(X_train_scaled, y_train, X_test_scaled)\n",
    "        model_used = \"PyOD-ECOD\"\n",
    "    else:\n",
    "        if method_fn is None:\n",
    "            raise ValueError(\n",
    "                \"use_baseline=False but no method_fn was provided. \"\n",
    "                \"Pass your own method wrapper, e.g. method_fn=your_method_scores.\"\n",
    "            )\n",
    "        scores = method_fn(X_train_scaled, y_train, X_test_scaled)\n",
    "        model_used = getattr(method_fn, \"__name__\", \"custom_method\")\n",
    "\n",
    "    metrics = evaluate_anomaly_scores(y_test, scores)\n",
    "\n",
    "    result: Dict[str, Any] = {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"n_train\": int(X_train.shape[0]),\n",
    "        \"n_test\": int(X_test.shape[0]),\n",
    "        \"model\": model_used,\n",
    "        \"metrics\": metrics,\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"\\nResults on dataset='{dataset_name}' using model='{model_used}':\\n\"\n",
    "        f\"  AUC-ROC : {metrics['auc_roc']:.4f}\\n\"\n",
    "        f\"  AUC-PR  : {metrics['auc_pr']:.4f}\\n\"\n",
    "        f\"  best F1 : {metrics['best_f1']:.4f} \"\n",
    "        f\"(at score threshold ≈ {metrics['best_f1_threshold']:.4f})\"\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Example call\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "example_result = run_single_dataset_example(\n",
    "    dataset_name=\"cardio\",   # or \"6\", \"6_cardio\", \"6_cardio.npz\"\n",
    "    use_baseline=False,       # use ECOD baseline\n",
    "    method_fn=your_method_scores,\n",
    ")\n",
    "\n",
    "example_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5102d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf4ad-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
